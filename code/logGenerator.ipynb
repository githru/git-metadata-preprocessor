{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log.json generator\n",
    "\n",
    "Complete notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:28:38.068917Z",
     "start_time": "2020-03-28T07:28:34.624984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Libraries...\n",
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/POSTecHyeoN/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing Libraries...\")\n",
    "# Import libraries!!!\n",
    "import subprocess\n",
    "import os, sys, re\n",
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import similarityCalLib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from math import log10\n",
    "from sklearn.manifold import TSNE\n",
    "from dateutil.parser import parse\n",
    "from dateutil import relativedelta\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:28:38.079733Z",
     "start_time": "2020-03-28T07:28:38.071022Z"
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "user_name = \"vuejs\"\n",
    "repo_name = \"vue\"\n",
    "api_url = \"https://api.github.com/\"\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:28:43.357343Z",
     "start_time": "2020-03-28T07:28:38.081761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLECTING LOGS...\n",
      "SUCCESS!!\n"
     ]
    }
   ],
   "source": [
    "print(\"COLLECTING LOGS...\")\n",
    "\n",
    "log_file_name = \"../log/\" + repo_name + \"-gitlog-all-parents-numstat-date.log\"\n",
    "reflog_file_name = \"../log/\" + repo_name + \"-reflog.log\"\n",
    "git_info_path = '../GIT-REPOS/' + repo_name + '/.git'\n",
    "\n",
    "\n",
    "log_file = open(log_file_name, \"w\")\n",
    "log_out = subprocess.check_output(['git', '--git-dir', git_info_path,'--no-pager', 'log', '--all', '--parents', '--numstat', '--date-order', '--pretty=fuller', '-c'], encoding=\"utf-8\")\n",
    "log_file.write(log_out)\n",
    "log_file.close()\n",
    "\n",
    "\n",
    "reflog_file = open(reflog_file_name, \"w\")\n",
    "reflog_out = subprocess.check_output(['git', '--git-dir', git_info_path,'--no-pager', 'log', '--simplify-by-decoration', '--tags', '--branches', '--remotes', '--date-order', '--decorate', '--pretty=tformat:\"%H    %C(auto)%D%Creset\"'], encoding=\"utf-8\")\n",
    "reflog_out = reflog_out.replace('\"',\"\")\n",
    "reflog_file.write(reflog_out)\n",
    "reflog_file.close()\n",
    "\n",
    "print(\"SUCCESS!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:28:43.924580Z",
     "start_time": "2020-03-28T07:28:43.359693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSING...\n",
      "GENERATING JSON...\n",
      "SUCCESS!!\n"
     ]
    }
   ],
   "source": [
    "commitDic = {}\n",
    "commitList = []\n",
    "commit = None\n",
    "\n",
    "print(\"PARSING...\")\n",
    "# parse log files\n",
    "no = 0\n",
    "for line in open(log_file_name, \"r\", encoding='UTF8'):\n",
    "    line = line.strip(\"\\n\")\n",
    "    if line.startswith(\"commit\"):\n",
    "        if commit != None:\n",
    "            commitList.append(commit)\n",
    "            commitDic[commit[\"id\"]] = commit\n",
    "            \n",
    "        commit = {}\n",
    "        items = line.split()\n",
    "        commit[\"no\"] = no\n",
    "        no = no + 1\n",
    "        commit[\"id\"] = items[1]\n",
    "        commit[\"parents\"] = items[2:]\n",
    "        commit[\"diffStat\"] = {\"changedFileCount\":0, \"insertions\":0, \"deletions\":0, \"files\": {}}\n",
    "        commit[\"message\"] = \"\"\n",
    "        continue\n",
    "    elif line.startswith(\"Merge\"):\n",
    "        commit[\"merge\"] = line.split()[1:]\n",
    "    elif line.startswith(\"Author:\"):\n",
    "        commit[\"author\"] = line.split(\"Author:     \")[1]\n",
    "    elif line.startswith(\"AuthorDate:\"):\n",
    "        commit[\"authorDate\"] = line.split(\"AuthorDate: \")[1]\n",
    "    elif line.startswith(\"Commit:\"):\n",
    "        commit[\"committer\"] = line.split(\"Commit:     \")[1]\n",
    "    elif line.startswith(\"CommitDate:\"):\n",
    "        commit[\"date\"] = line.split(\"CommitDate: \")[1]\n",
    "    elif line.startswith(\"    \"):\n",
    "        line = line.strip()\n",
    "        if line != \"\" and commit[\"message\"] != \"\":\n",
    "            commit[\"message\"] += \"\\n\"\n",
    "        if line.startswith(\"Signed-off-by:\"):\n",
    "            continue\n",
    "        commit[\"message\"] += line.strip()\n",
    "        commit[\"message\"] = commit[\"message\"].strip()\n",
    "    else: #log\n",
    "        if line != \"\":\n",
    "            [ins, dels, fileName] = line.split(\"\\t\")\n",
    "            if ins == \"-\":\n",
    "                ins = 0\n",
    "                dels = 0\n",
    "            else:\n",
    "                ins = int(ins)\n",
    "                dels = int(dels)\n",
    "            commit[\"diffStat\"][\"changedFileCount\"] += 1\n",
    "            commit[\"diffStat\"][\"files\"][fileName] = {\"insertions\":ins, \"deletions\":dels};\n",
    "            commit[\"diffStat\"][\"insertions\"] += ins\n",
    "            commit[\"diffStat\"][\"deletions\"] += dels\n",
    "commitList.append(commit)\n",
    "commitDic[commit[\"id\"]] = commit\n",
    "\n",
    "for line in open(reflog_file_name, \"r\", encoding='UTF8'):\n",
    "    line = line.strip(\"\\n\").strip()\n",
    "    items = line.split(\"    \")\n",
    "    if len(items) > 1:\n",
    "        branches = []\n",
    "        tags = []\n",
    "        refs = items[1].split(\", \")\n",
    "        for ref in refs:\n",
    "            if ref.startswith(\"tag: \"):\n",
    "                tags.append(ref[5:])\n",
    "            elif ref.startswith(\"HEAD -> \"):\n",
    "                commitDic[items[0]][\"isHead\"] = True\n",
    "                tags.append(ref[8:])\n",
    "            else:\n",
    "                branches.append(ref)\n",
    "        commitDic[items[0]][\"branches\"] = branches\n",
    "        commitDic[items[0]][\"tags\"] = tags\n",
    "        \n",
    "print(\"GENERATING JSON...\")\n",
    "\n",
    "json_log_file_name = log_file_name + \".json\"\n",
    "\n",
    "# generate json file        \n",
    "json_file = open(json_log_file_name, \"w\")\n",
    "json_file.write(json.dumps(commitList, indent=4, separators=(',', ': ')))\n",
    "json_file.close()\n",
    "\n",
    "print(\"SUCCESS!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling issues / PR infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:34:10.022709Z",
     "start_time": "2020-03-28T07:33:08.320914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently crawling pulls...\n",
      "Crawled https://api.github.com/repos/vuejs/vue/pulls/11346\n",
      "Crawled https://api.github.com/repos/vuejs/vue/pulls/11349\n",
      "Crawled https://api.github.com/repos/vuejs/vue/pulls/11351\n",
      "PR Crawling finished!!!\n",
      "Crawling Finished!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def retreive_rate(access_token):\n",
    "    r = requests.get(api_url + \"rate_limit\" + access_token)\n",
    "    if(r.ok):\n",
    "        rateItem = json.loads(r.text or r.content)\n",
    "        return rateItem[\"resources\"][\"core\"][\"remaining\"]\n",
    "#     else:\n",
    "#         quit()\n",
    "        \n",
    "def store_temp(info, info_str):\n",
    "    with open(\"../log/\"+ repo_name +\".\" + info_str + \"_temp.json\",\"w\", encoding=\"utf-8\") as info_json:\n",
    "        json.dump(info, info_json, indent=\"\\t\")\n",
    "\n",
    "def store(info, info_str, temp_exists = True):\n",
    "    if(info_str == \"pulls\"):\n",
    "        info_str = \"pulls_raw\"\n",
    "    with open(\"../log/\"+ repo_name + \".\" + info_str + \".json\",\"w\", encoding=\"utf-8\") as info_json:\n",
    "        json.dump(info, info_json, indent=\"\\t\")\n",
    "    if temp_exists:\n",
    "        os.remove(\"../log/\"+ repo_name +\".\" + info_str + \"_temp.json\") if os.path.isfile(\"../log/\"+ repo_name +\".\" + info_str + \"_temp.json\") else None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## Get Access Token (stored in .gitignore)\n",
    "with open('./token.txt', \"r\") as token_file:\n",
    "    access_token = \"?access_token=\" + token_file.readline()\n",
    "    access_token = access_token.rstrip()\n",
    "    \n",
    "\n",
    "# issue / issues -> info\n",
    "def crawler(info_str): \n",
    "    info = []\n",
    "    finished = False\n",
    "    \n",
    "    base_url = api_url + \"repos/\" + user_name + \"/\" + repo_name + \"/\" + info_str + \"/\"\n",
    "    end_url = \"/comments\"\n",
    "\n",
    "    try:\n",
    "        with open(\"../log/\"+ repo_name +\".\" + info_str + \"_temp.json\", \"r\", encoding=\"utf-8\") as info_json:\n",
    "            info = json.load(info_json)\n",
    "    except:\n",
    "        print(\"Starting issue crawling...\")\n",
    "\n",
    "    idx_current = len(info)\n",
    "    i = idx_current\n",
    "    \n",
    "    issue_num = None\n",
    "    \n",
    "    issue_r = requests.get(base_url[:-1] + access_token + \"&state=all\")\n",
    "    if(issue_r.text == \"[]\"):\n",
    "        store([], info_str)\n",
    "        return\n",
    "        \n",
    "    \n",
    "    if(issue_r.ok):\n",
    "        repoItem = json.loads(issue_r.text or issue_r.content)\n",
    "        issue_num = int(repoItem[0][\"url\"].split(\"/\")[-1])\n",
    "        print(\"Total Issue Number: \" + str(issue_num))\n",
    "        \n",
    "    try:  \n",
    "        while True:\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                clear_output()\n",
    "                print(\"Currently crawling \" + info_str + \"...\")\n",
    "            print(\"Crawled \" + base_url + str(i)) if(i % 10 == 0 or i == 1) else None\n",
    "\n",
    "            r = requests.get(base_url + str(i) + access_token)\n",
    "            if(r.ok):  # Success!!\n",
    "                repoItem = json.loads(r.text or r.content)\n",
    "                info.append(repoItem)\n",
    "            else:      # Failed!!\n",
    "                store_temp(info, info_str)  ## First of all, store current info\n",
    "                if(i == issue_num + 1):\n",
    "                    store(info,info_str)\n",
    "                if(retreive_rate(access_token) < 3):  # Maybe rate limit exceed\n",
    "                    while True:\n",
    "                        print(\"Wait until the api rate restores...[3 minutes]\")\n",
    "                        time.sleep(180)\n",
    "                        remaining_rate = retreive_rate(access_token)              \n",
    "                        print(\"Remaining API Rate: \" + str(remaining_rate) + \" times\")\n",
    "                        if(remaining_rate > 2000):\n",
    "                            break\n",
    "                else:\n",
    "                    if i > issue_num:\n",
    "                        finished = True\n",
    "                        break\n",
    "    except:\n",
    "        store_temp(info, info_str)\n",
    "\n",
    "    if  finished:\n",
    "        print(\"Issue Crawling finished!!!\")\n",
    "        store(info, info_str)\n",
    "    else:\n",
    "        print(\"Not finished yet...\\nCurrent issue #: \" + str(len(info)))\n",
    "        store_temp(info, info_str)\n",
    "        remaining_rate = retreive_rate(access_token)              \n",
    "        print(\"Remaining API Rate: \" + str(remaining_rate) + \" times\")\n",
    "\n",
    "### Crawling Issues\n",
    "\n",
    "if (not os.path.isfile(\"../log/\"+ repo_name +\".issues.json\")):\n",
    "    crawler(\"issues\")\n",
    "\n",
    "### Crawling pull infos\n",
    "\n",
    "if (os.path.isfile(\"../log/\"+ repo_name +\".issues.json\") and not os.path.isfile(\"../log/\"+ repo_name +\".pulls_html_temp.json\") and not os.path.isfile(\"../log/\" + repo_name + \".pulls.json\")):\n",
    "    print(\"Extracting PR infos from issues.json\")\n",
    "    with open(\"../log/\"+ repo_name +\".issues.json\", \"r\", encoding=\"utf-8\") as issue_json:\n",
    "        issues = json.load(issue_json)\n",
    "        pulls_html = []\n",
    "        for issue in issues:\n",
    "            if \"pull_request\" in issue.keys():\n",
    "                  pulls_html.append(issue[\"html_url\"])\n",
    "        print(\"Storing PR data...\")        \n",
    "        store_temp(pulls_html,\"pulls_html\")\n",
    "        print(\"PR html url Temporary Crawling finished!!\")\n",
    "\n",
    "if(os.path.isfile(\"../log/\" + repo_name + \".pulls_html_temp.json\") and not os.path.isfile(\"../log/\" + repo_name + \".pulls.json\")):\n",
    "    with open(\"../log/\" + repo_name + \".pulls_html_temp.json\", \"r\", encoding=\"utf-8\") as pulls_htmls_json:\n",
    "        pulls_htmls = json.load(pulls_htmls_json)\n",
    "        pulls = []\n",
    "        pulls_finished = False\n",
    "        try:\n",
    "            with open(\"../log/\" + repo_name + \".pulls_temp.json\", \"r\", encoding=\"utf-8\") as pulls_json:\n",
    "                pulls = json.load(pulls_json)\n",
    "        except:\n",
    "            print(\"Starting pull crawling...\")\n",
    "            print(\"PR Total #: \" + str(len(pulls_htmls)))\n",
    "            \n",
    "        current_num = -1\n",
    "        if len(pulls) > 0:\n",
    "            current_num = pulls[-1][\"number\"]\n",
    "        \n",
    "        \n",
    "        base_url_pulls = api_url + \"repos/\" + user_name + \"/\" + repo_name + \"/pulls/\"\n",
    "        try:    \n",
    "            for (idx, html) in enumerate(pulls_htmls):\n",
    "                \n",
    "                if(idx % 10 == 0 and idx > 5):\n",
    "                    clear_output()\n",
    "                    print(\"Currently crawling pulls...\")\n",
    "\n",
    "                if current_num < int(html.split(\"/\")[-1]):\n",
    "                    r = requests.get(base_url_pulls + html.split(\"/\")[-1] + access_token)\n",
    "                    if(r.ok):  # Success!!\n",
    "                        print(\"Crawled \" + base_url_pulls + html.split(\"/\")[-1])\n",
    "                        repoItem = json.loads(r.text or r.content)\n",
    "                        pulls.append(repoItem)\n",
    "                    else:      # Failed!!\n",
    "                        store_temp(pulls, \"pulls\")  ## First of all, store current info\n",
    "                        if(retreive_rate(access_token) < 3):  # Maybe rate limit exceed\n",
    "                            while True:\n",
    "                                print(\"Wait until the api rate restores...[3 minutes]\")\n",
    "                                time.sleep(180)\n",
    "                                remaining_rate = retreive_rate(access_token)              \n",
    "                                print(\"Remaining API Rate: \" + str(remaining_rate) + \" times\")\n",
    "                                if(remaining_rate > 2000):\n",
    "                                    break\n",
    "                        else:\n",
    "                            pulls_finished = True\n",
    "                            break\n",
    "                if idx == len(pulls_htmls) - 1:\n",
    "                    pulls_finished = True\n",
    "        except:\n",
    "            store_temp(pulls, \"pulls\")   \n",
    "            \n",
    "        if  pulls_finished:\n",
    "            print(\"PR Crawling finished!!!\")\n",
    "            store(pulls, \"pulls\")\n",
    "            os.remove(\"../log/\" + repo_name + \".pulls_html_temp.json\")\n",
    "        else:\n",
    "            print(\"Not finished yet...\\nCurrent pull #: \" + str(len(pulls)) + \" / \" + str(len(pulls_htmls)))\n",
    "            store_temp(pulls, \"pulls\")\n",
    "            remaining_rate = retreive_rate(access_token)              \n",
    "            print(\"Remaining API Rate: \" + str(remaining_rate) + \" times\")\n",
    "            \n",
    "        if (pulls_htmls == []):\n",
    "            store([], \"pulls\")\n",
    "                                          \n",
    "print(\"Crawling Finished!!!!!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing raw PR data to compact PR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:29:55.113344Z",
     "start_time": "2020-03-28T07:29:55.100056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing data...and\n",
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"reducing data...and\")\n",
    "\n",
    "with open(\"../log/\" + repo_name + \".pulls_raw.json\", \"r\", encoding=\"utf-8\") as pulls_json:\n",
    "    raw_pulls = json.load(pulls_json)\n",
    "    pulls_compact_data = []\n",
    "    for item in raw_pulls:\n",
    "        newItem = {}\n",
    "        newItem[\"number\"] = item[\"number\"]\n",
    "        newItem[\"state\"] = item[\"state\"]\n",
    "        newItem[\"title\"] = item[\"title\"]\n",
    "        newItem[\"body\"] = item[\"body\"]\n",
    "        newItem[\"message\"] = item[\"title\"] if item[\"body\"] == None else item[\"title\"] + \" \" + item[\"body\"]\n",
    "        newItem[\"merge_commit_sha\"] = item[\"merge_commit_sha\"]\n",
    "        newItemHead = {}\n",
    "        newItemHead[\"sha\"] = item[\"head\"][\"sha\"]\n",
    "        newItem[\"head\"] = newItemHead\n",
    "        newItemBase = {}\n",
    "        newItemBase[\"sha\"] = item[\"base\"][\"sha\"]\n",
    "        newItem[\"base\"] = newItemBase\n",
    "        newItem[\"commitsLink\"] = item[\"_links\"][\"commits\"][\"href\"]\n",
    "        newItem[\"merged\"] = item[\"merged\"]\n",
    "        pulls_compact_data.append(newItem)\n",
    "    \n",
    "    with open(\"../log/\"+ repo_name + \".\" + \"pulls_compress.json\",\"w\", encoding=\"utf-8\") as info_json:\n",
    "        json.dump(pulls_compact_data, info_json, indent=\"\\t\")\n",
    "        \n",
    "print(\"finished!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Analysis step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:29:56.326197Z",
     "start_time": "2020-03-28T07:29:55.115046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP ANALYSIS\n",
      "dictionary size : 7763\n",
      "filtered dictionary size : 7488\n",
      "dictionary size : 7585\n",
      "filtered dictionary size : 7340\n",
      "SUCCESS!!\n"
     ]
    }
   ],
   "source": [
    "log_file_name = \"../log/\" + repo_name + \"-gitlog-all-parents-numstat-date.log\"\n",
    "json_log_file_name = log_file_name + \".json\"\n",
    "\n",
    "custom_stopwords = [',', '.', '\\'','-','<','>'] # to be added\n",
    "single_characters = list(\"abcdefghijklnmopqrstuvwxyz\")\n",
    "custom_stopwords = stopwords.words(('english'))+ custom_stopwords + single_characters\n",
    "\n",
    "def commit_analysis(filename, outfile):\n",
    "    with open(filename) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        for commit in json_data:\n",
    "            keyword_match = False\n",
    "            for keyword in keyword_list:\n",
    "                if commit[\"message\"].lower().find(keyword) != -1:\n",
    "                    commit[\"commitType\"] = keyword_dict[keyword]\n",
    "                    keyword_match =  True\n",
    "                    break\n",
    "            if not keyword_match:\n",
    "                commit[\"commitType\"] = \"not_mapped\"\n",
    "\n",
    "        #os.remove(filename)\n",
    "    with open(outfile, \"w\") as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "        \n",
    "    topic_analysis(outfile, \"message\")\n",
    "\n",
    "def topic_analysis(filename, typeName):\n",
    "    min_count = 40\n",
    "    commit_list = []\n",
    "    with open(filename) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        for commit in json_data:\n",
    "            corpus = re.sub('[^a-zA-Z0-9_#\\.\\-]', ' ', commit[typeName].lower().replace(\"\\'s\", \" \"))\n",
    "            corpus = list(corpus)\n",
    "            for (idx, char) in enumerate(corpus):\n",
    "                if(corpus[idx] == \"#\"):\n",
    "                    try:\n",
    "                        num = int(corpus[idx + 1])\n",
    "                    except:\n",
    "                        corpus[idx] = \" \"\n",
    "            \n",
    "            corpus = \"\".join(corpus)\n",
    "            filtered_corpus = [word for word in corpus.split() if word not in custom_stopwords]\n",
    "\n",
    "            filtered_corpus = list(map(lambda x: x if x[-1] != \".\" else x[:-1], filtered_corpus))\n",
    "            # stopwords.words('english')\n",
    "            commit[\"corpus\"] = filtered_corpus\n",
    "            commit_list.append(filtered_corpus)\n",
    "    os.remove(filename)\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(commit_list)\n",
    "    print('dictionary size : %d' % len(dictionary))\n",
    "        \n",
    "    word_counter = Counter((word for words in commit_list for word in words))\n",
    "        \n",
    "    removal_word_idxs = { dictionary.token2id[word] for word, count in word_counter.items() if count > min_count }\n",
    "    #print(removal_word_idxs)\n",
    "    dictionary.filter_tokens(removal_word_idxs)\n",
    "    dictionary.compactify()\n",
    "    print('filtered dictionary size : %d' % len(dictionary))\n",
    "        \n",
    "    \"\"\"\n",
    "        for k,v in dictionary.token2id.items():\n",
    "                print(k,v)  # print key(word), value(idx) pair in filtered dictionary\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "        common_corpus = [dictionary.doc2bow(text) for text in commit_list]\n",
    "        lda = LdaModel(common_corpus, num_topics=10, id2word=dictionary)\n",
    "        for topic in lda.print_topics():\n",
    "                print(topic)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "keyword_list = [\"implement\", \"add\", \"request\", \"new\", \"test\", \"start\", \"includ\", \"initial\", \"introduc\", \"creat\", \"increas\", \n",
    "\"optimiz\", \"adjust\", \"update\", \"delet\", \"remov\", \"chang\", \"refactor\", \"replac\", \"modif\", \"is now\", \"are now\", \n",
    "\"enhance\", \"improv\", \"design\", \"change\", \"renam\", \"eliminat\", \"duplicat\", \"restrutur\", \"simplif\", \"obsolete\", \n",
    "\"rearrang\", \"miss\", \"enhanc\", \"improv\", \"bug\", \"x\", \"issue\", \"error\", \"correct\", \"proper\", \"deprecat\", \"broke\",\n",
    "\"clean\", \"license\", \"merge\", \"release\", \"structure\", \"integrat\", \"copyright\", \"documentation\", \n",
    "\"manual\", \"javadoc\", \"comment\", \"migrat\", \"repository\", \"code review\", \"polish\", \"upgrade\", \"style\", \"formatting\", \"organiz\", \"todo\"]\n",
    "keyword_dict = dict()\n",
    "\n",
    "class_label = \"forward_engineering\"\n",
    "for keyword in keyword_list:\n",
    "        if keyword == \"optimiz\":\n",
    "                class_label = \"reengineering\"\n",
    "        elif keyword == \"bug\":\n",
    "                class_label = \"corrective_engineering\"\n",
    "        elif keyword == \"clean\":\n",
    "                class_label = \"management\"\n",
    "        keyword_dict[keyword] = class_label\n",
    "\n",
    "\"\"\"\n",
    "keyword_dict[\"forward_engineering\"] = [\"implement\", \"add\", \"request\", \"new\", \"test\", \"start\", \n",
    "\"includ\", \"initial\", \"introduc\", \"creat\", \"increas\"]\n",
    "keyword_dict[\"reengineering\"] = [\"optimiz\", \"adjust\", \"update\", \"delet\", \"remov\", \"chang\", \"refactor\", \"replac\", \"modif\", \"is now\", \"are now\", \n",
    "\"enhance\", \"improv\", \"design\", \"change\", \"renam\", \"eliminat\", \"duplicat\", \"restrutur\", \"simplif\", \"obsolete\", \"rearrang\", \"miss\", \"enhanc\", \"improv\"]\n",
    "keyword_dict[\"corrective_engineering\"] = [\"bug\", \"x\", \"issue\", \"error\", \"correct\", \"proper\", \"deprecat\", \"broke\"]\n",
    "keyword_dict[\"management\"] = [\"clean\", \"license\", \"merge\", \"release\", \"structure\", \"integrat\", \"copyright\", \"documentation\", \n",
    "\"manual\", \"javadoc\", \"comment\", \"migrat\", \"repository\", \"code review\", \"polish\", \"upgrade\", \"style\", \"formatting\", \"organiz\", \"todo\"]\n",
    "\"\"\"\n",
    "\n",
    "print(\"NLP ANALYSIS\")\n",
    "commit_analysis(json_log_file_name, \"../log/\" + repo_name + \".nlp.json\")\n",
    "\n",
    "topic_analysis(\"../log/\"+ repo_name + \".\" + \"pulls_compress.json\", \"message\")\n",
    "print(\"SUCCESS!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting pull / issue info with commit history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:30:05.029825Z",
     "start_time": "2020-03-28T07:29:56.328518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding issue info to the commit history\n",
      "Adding PR info to the commit history\n",
      "Total pull #: 1793\n",
      "Pull #0 handled\n",
      "Pull #10 handled\n",
      "Pull #20 handled\n",
      "Pull #30 handled\n",
      "Pull #40 handled\n",
      "Pull #50 handled\n",
      "Pull #60 handled\n",
      "Pull #70 handled\n",
      "Pull #80 handled\n",
      "Pull #90 handled\n",
      "Pull #100 handled\n",
      "Pull #110 handled\n",
      "Pull #120 handled\n",
      "Pull #130 handled\n",
      "Pull #140 handled\n",
      "Pull #150 handled\n",
      "Pull #160 handled\n",
      "Pull #170 handled\n",
      "Pull #180 handled\n",
      "Pull #190 handled\n",
      "Pull #200 handled\n",
      "Pull #210 handled\n",
      "Pull #220 handled\n",
      "Pull #230 handled\n",
      "Pull #240 handled\n",
      "Pull #250 handled\n",
      "Pull #260 handled\n",
      "Pull #270 handled\n",
      "Pull #280 handled\n",
      "Pull #290 handled\n",
      "Pull #300 handled\n",
      "Pull #310 handled\n",
      "Pull #320 handled\n",
      "Pull #330 handled\n",
      "Pull #340 handled\n",
      "Pull #350 handled\n",
      "Pull #360 handled\n",
      "Pull #370 handled\n",
      "Pull #380 handled\n",
      "Pull #390 handled\n",
      "Pull #400 handled\n",
      "Pull #410 handled\n",
      "Pull #420 handled\n",
      "Pull #430 handled\n",
      "Pull #440 handled\n",
      "Pull #450 handled\n",
      "Pull #460 handled\n",
      "Pull #470 handled\n",
      "Pull #480 handled\n",
      "Pull #490 handled\n",
      "Pull #500 handled\n",
      "Pull #510 handled\n",
      "Pull #520 handled\n",
      "Pull #530 handled\n",
      "Pull #540 handled\n",
      "Pull #550 handled\n",
      "Pull #560 handled\n",
      "Pull #570 handled\n",
      "Pull #580 handled\n",
      "Pull #590 handled\n",
      "Pull #600 handled\n",
      "Pull #610 handled\n",
      "Pull #620 handled\n",
      "Pull #630 handled\n",
      "Pull #640 handled\n",
      "Pull #650 handled\n",
      "Pull #660 handled\n",
      "Pull #670 handled\n",
      "Pull #680 handled\n",
      "Pull #690 handled\n",
      "Pull #700 handled\n",
      "Pull #710 handled\n",
      "Pull #720 handled\n",
      "Pull #730 handled\n",
      "Pull #740 handled\n",
      "Pull #750 handled\n",
      "Pull #760 handled\n",
      "Pull #770 handled\n",
      "Pull #780 handled\n",
      "Pull #790 handled\n",
      "Pull #800 handled\n",
      "Pull #810 handled\n",
      "Pull #820 handled\n",
      "Pull #830 handled\n",
      "Pull #840 handled\n",
      "Pull #850 handled\n",
      "Pull #860 handled\n",
      "Pull #870 handled\n",
      "Pull #880 handled\n",
      "Pull #890 handled\n",
      "Pull #900 handled\n",
      "Pull #910 handled\n",
      "Pull #920 handled\n",
      "Pull #930 handled\n",
      "Pull #940 handled\n",
      "Pull #950 handled\n",
      "Pull #960 handled\n",
      "Pull #970 handled\n",
      "Pull #980 handled\n",
      "Pull #990 handled\n",
      "Pull #1000 handled\n",
      "Pull #1010 handled\n",
      "Pull #1020 handled\n",
      "Pull #1030 handled\n",
      "Pull #1040 handled\n",
      "Pull #1050 handled\n",
      "Pull #1060 handled\n",
      "Pull #1070 handled\n",
      "Pull #1080 handled\n",
      "Pull #1090 handled\n",
      "Pull #1100 handled\n",
      "Pull #1110 handled\n",
      "Pull #1120 handled\n",
      "Pull #1130 handled\n",
      "Pull #1140 handled\n",
      "Pull #1150 handled\n",
      "Pull #1160 handled\n",
      "Pull #1170 handled\n",
      "Pull #1180 handled\n",
      "Pull #1190 handled\n",
      "Pull #1200 handled\n",
      "Pull #1210 handled\n",
      "Pull #1220 handled\n",
      "Pull #1230 handled\n",
      "Pull #1240 handled\n",
      "Pull #1250 handled\n",
      "Pull #1260 handled\n",
      "Pull #1270 handled\n",
      "Pull #1280 handled\n",
      "Pull #1290 handled\n",
      "Pull #1300 handled\n",
      "Pull #1310 handled\n",
      "Pull #1320 handled\n",
      "Pull #1330 handled\n",
      "Pull #1340 handled\n",
      "Pull #1350 handled\n",
      "Pull #1360 handled\n",
      "Pull #1370 handled\n",
      "Pull #1380 handled\n",
      "Pull #1390 handled\n",
      "Pull #1400 handled\n",
      "Pull #1410 handled\n",
      "Pull #1420 handled\n",
      "Pull #1430 handled\n",
      "Pull #1440 handled\n",
      "Pull #1450 handled\n",
      "Pull #1460 handled\n",
      "Pull #1470 handled\n",
      "Pull #1480 handled\n",
      "Pull #1490 handled\n",
      "Pull #1500 handled\n",
      "Pull #1510 handled\n",
      "Pull #1520 handled\n",
      "Pull #1530 handled\n",
      "Pull #1540 handled\n",
      "Pull #1550 handled\n",
      "Pull #1560 handled\n",
      "Pull #1570 handled\n",
      "Pull #1580 handled\n",
      "Pull #1590 handled\n",
      "Pull #1600 handled\n",
      "Pull #1610 handled\n",
      "Pull #1620 handled\n",
      "Pull #1630 handled\n",
      "Pull #1640 handled\n",
      "Pull #1650 handled\n",
      "Pull #1660 handled\n",
      "Pull #1670 handled\n",
      "Pull #1680 handled\n",
      "Pull #1690 handled\n",
      "Pull #1700 handled\n",
      "Pull #1710 handled\n",
      "Pull #1720 handled\n",
      "Pull #1730 handled\n",
      "Pull #1740 handled\n",
      "Pull #1750 handled\n",
      "Pull #1760 handled\n",
      "Pull #1770 handled\n",
      "Pull #1780 handled\n",
      "Pull #1790 handled\n",
      "Printing result...\n",
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "with open('./token.txt', \"r\") as token_file:\n",
    "    access_token = \"?access_token=\" + token_file.readline()\n",
    "\n",
    "def add_issue():\n",
    "    origin_file_name = \"../log/\" + repo_name + \".nlp.json\"\n",
    "    \n",
    "    \n",
    "    print(\"Adding issue info to the commit history\")\n",
    "    \n",
    "    with open(origin_file_name) as origin_commit_file:\n",
    "        origin_commits = json.load(origin_commit_file)\n",
    "        for commit in origin_commits:\n",
    "            message = commit[\"message\"]\n",
    "            issue_reg = re.compile(\"#\\d+\")\n",
    "            m = issue_reg.findall(message)\n",
    "            \n",
    "            related_issues = []            \n",
    "            if m:\n",
    "                for issue in m:\n",
    "                    related_issues.append(issue[1:])\n",
    "            \n",
    "            commit[\"issues\"] = related_issues\n",
    "        \n",
    "    \n",
    "        return origin_commits\n",
    "\n",
    "def add_pull(origin_commits):\n",
    "    origin_pull_file_name = \"../log/\" + repo_name + \".pulls_compress.json\"\n",
    "    final_file_name = \"../log/\" + repo_name + \".nlp.withissue.json\"\n",
    "    \n",
    "    sha2Index = {}\n",
    "    \n",
    "    for (idx, commit) in enumerate(origin_commits):\n",
    "        sha2Index[commit[\"id\"]] = idx\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    print(\"Adding PR info to the commit history\")\n",
    "    with open(origin_pull_file_name) as pull_info_file:\n",
    "        pulls_info = json.load(pull_info_file)\n",
    "        \n",
    "        print(\"Total pull #: \" + str(len(pulls_info)))\n",
    "        for (idx, pull) in enumerate(pulls_info):\n",
    "            link = pull[\"commitsLink\"]\n",
    "            r = requests.get(link + access_token)\n",
    "            if (r.ok):\n",
    "                repoItem = json.loads(r.text or r.content)\n",
    "                for commit_info in repoItem:\n",
    "                    try:\n",
    "                        index = sha2Index[commit_info[\"sha\"]]\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "                    if \"pulls\" not in origin_commits[index].keys():\n",
    "                        origin_commits[index][\"pulls\"] = [int(pull[\"number\"])]\n",
    "                    else:\n",
    "                        origin_commits[index][\"pulls\"].append(int(pull[\"number\"]))\n",
    "            else:\n",
    "                while True:\n",
    "                    print(\"Wait until the api rate restores...[3 minutes]\")\n",
    "                    time.sleep(180)\n",
    "                    remaining_rate = retreive_rate(access_token)              \n",
    "                    print(\"Remaining API Rate: \" + str(remaining_rate) + \" times\")\n",
    "                    if(remaining_rate > 2000):\n",
    "                        break\n",
    "            if idx % 10 == 0:\n",
    "                print(\"Pull #\" + str(idx) + \" handled\")\n",
    "\n",
    "\n",
    "    print(\"Printing result...\")\n",
    "    final_file = open(final_file_name, \"w\")\n",
    "    final_file.write(json.dumps(origin_commits, indent=4, separators=(',', ': ')))\n",
    "    print(\"finished!!\")\n",
    "\n",
    "                       \n",
    "        \n",
    "add_pull(add_issue())\n",
    "os.remove(\"../log/\" + repo_name + \".nlp.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding tf/idf info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:30:05.689493Z",
     "start_time": "2020-03-28T07:30:05.032698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess!!\n"
     ]
    }
   ],
   "source": [
    "# Global variable for corpus\n",
    "corpusDfDict = {}\n",
    "\n",
    "\n",
    "class tfIdfGenerator:\n",
    "    \n",
    "                \n",
    "    commitList = None\n",
    "    jsonData = None\n",
    "    \n",
    "    def __init__(self, jsonData):\n",
    "        \n",
    "        self.commitList = []\n",
    "        self.jsonData = jsonData\n",
    "\n",
    "        for commit in jsonData:\n",
    "            self.commitList.append(commit) # for reserving the order of commits   \n",
    "        commitNum = len(self.commitList)\n",
    "        \n",
    "        for idx, commit in enumerate(self.commitList): \n",
    "            currentCommitWords = []\n",
    "            for word in commit[\"corpus\"]:\n",
    "                if word not in currentCommitWords:\n",
    "                    if word not in corpusDfDict:\n",
    "                        corpusDfDict[word] = [len(corpusDfDict), 1]  # idx / idf list\n",
    "                    else:\n",
    "                        corpusDfDict[word][1] += 1\n",
    "                    currentCommitWords.append(word)\n",
    "\n",
    "        for key in corpusDfDict.keys():\n",
    "            corpusDfDict[key][1] = log10(commitNum / corpusDfDict[key][1])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def addTfIdfInfo(self):\n",
    "        \n",
    "        \n",
    "        for commit in self.jsonData:\n",
    "            currentIdx2TfIdf = {}\n",
    "            for word in commit[\"corpus\"]:\n",
    "                idx = corpusDfDict[word][0]\n",
    "                idf = corpusDfDict[word][1]\n",
    "                if idx not in currentIdx2TfIdf.keys():\n",
    "                    currentIdx2TfIdf[idx] = idf\n",
    "                else:\n",
    "                    currentIdx2TfIdf[idx] += idf\n",
    "            if(len(currentIdx2TfIdf) > 0):\n",
    "                commit[\"tfidf\"] = currentIdx2TfIdf\n",
    "            else:\n",
    "                currentIdx2TfIdf[-1] = 0\n",
    "                commit[\"tfidf\"] = currentIdx2TfIdf\n",
    "                    \n",
    "        return self.jsonData\n",
    "    \n",
    "            \n",
    "            \n",
    "with open(\"../log/\" + repo_name + \".nlp.withissue.json\") as jsonCommitFile:\n",
    "    jsonCommitData = json.load(jsonCommitFile)      \n",
    "    tfidfCommit = tfIdfGenerator(jsonCommitData)\n",
    "    \n",
    "    \n",
    "with open(\"../log/\" + repo_name + \".pulls_compress.json\") as jsonPullFile:\n",
    "    jsonPullData = json.load(jsonPullFile)\n",
    "    tfidfPull = tfIdfGenerator(jsonPullData)\n",
    "    \n",
    "    \n",
    "idx2corpus = []\n",
    "\n",
    "for word in corpusDfDict:\n",
    "   idx2corpus.append(word)\n",
    "\n",
    "json_idx2corpus_file = open(\"../log/\" + repo_name + \".corpus.json\", \"w\")\n",
    "json_idx2corpus_file.write(json.dumps(idx2corpus, indent=4, separators=(',', ': ')))\n",
    "json_idx2corpus_file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "jsonWithTfidfCommit = tfidfCommit.addTfIdfInfo()\n",
    "\n",
    "commitFile = open(\"../log/\" + repo_name + \".commits.json\", \"w\")\n",
    "commitFile.write(json.dumps(jsonWithTfidfCommit, indent=4, separators=(',', ': ')))\n",
    "commitFile.close()\n",
    "\n",
    "\n",
    "jsonWithTfidfPull = tfidfPull.addTfIdfInfo()\n",
    "\n",
    "pullFile = open(\"../log/\" + repo_name + \".pulls.json\", \"w\")\n",
    "pullFile.write(json.dumps(jsonWithTfidfPull, indent=4, separators=(',', ': ')))\n",
    "pullFile.close()\n",
    "\n",
    "print(\"Sucess!!\")\n",
    "\n",
    "\n",
    "# os.remove(\"../log/\" + repo_name + \".nlp.withissue.json\")\n",
    "# os.remove(\"../log/\" + repo_name + \"-reflog.log\")\n",
    "# os.remove(\"../log/\" + repo_name + \"-gitlog-all-parents-numstat-date.log\")\n",
    "# os.remove(\"../log/\" + repo_name + \"-gitlog-all-parents-numstat-date.log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing for similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:30:06.020205Z",
     "start_time": "2020-03-28T07:30:05.691857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit History Length:  10521\n",
      "HEAD IS NOT ORIGIN/MASTER OR MASTER\n",
      "\n",
      "Calculating pair length...\n",
      "iter 0 finished\n",
      "iter 400 finished\n",
      "iter 800 finished\n",
      "iter 1200 finished\n",
      "iter 1600 finished\n",
      "iter 2000 finished\n",
      "iter 2400 finished\n",
      "iter 2800 finished\n",
      "iter 3200 finished\n",
      "iter 3600 finished\n",
      "iter 4000 finished\n",
      "iter 4400 finished\n",
      "iter 4800 finished\n",
      "iter 5200 finished\n",
      "iter 5600 finished\n",
      "iter 6000 finished\n",
      "iter 6400 finished\n",
      "iter 6800 finished\n",
      "iter 7200 finished\n",
      "iter 7600 finished\n",
      "iter 8000 finished\n",
      "iter 8400 finished\n",
      "iter 8800 finished\n",
      "iter 9200 finished\n",
      "iter 9600 finished\n",
      "iter 10000 finished\n",
      "iter 10400 finished\n",
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### Exporting json data ###\n",
    "###########################\n",
    "\n",
    "jsonFileName = \"../log/\" + repo_name + \".commits.json\"\n",
    "\n",
    "\n",
    "with open(jsonFileName) as jsonFile:\n",
    "    jsonData = json.load(jsonFile)\n",
    "\n",
    "print(\"Commit History Length: \", len(jsonData))\n",
    "\n",
    "################################################\n",
    "### Constructing commit history BRANCH graph ###\n",
    "################################################\n",
    "\n",
    "dags = nx.Graph()\n",
    "firstParentDags = nx.DiGraph()\n",
    "\n",
    "commitDic = {}\n",
    "commitList = []\n",
    "no = 0\n",
    "headId = \"\"\n",
    "\n",
    "# sorted in back order\n",
    "for commit in jsonData:\n",
    "    commitList.append(commit) # for reserving the order of commits\n",
    "    commitDic[commit[\"id\"]] = commit\n",
    "    \n",
    "    dags.add_node(commit[\"id\"])\n",
    "    firstParentDags.add_node(commit[\"id\"])\n",
    "    map(dags.add_node, commit[\"parents\"])\n",
    "    firstParentDags.add_node(commit[\"parents\"][0]) if len(commit[\"parents\"]) > 0 else None\n",
    "    \n",
    "#     map(dags.add_edge, [[commit[\"id\"], p] for p in commit[\"parents\"]])\n",
    "    isFirstParent = True\n",
    "    for p in commit[\"parents\"]:\n",
    "        dags.add_edge(p, commit[\"id\"])\n",
    "        if isFirstParent:\n",
    "            firstParentDags.add_edge(p, commit[\"id\"])\n",
    "            isFirstParent = False\n",
    "    \n",
    "    if \"isHead\" in commit:\n",
    "        headId = commit[\"id\"]\n",
    "        \n",
    "#####################################        \n",
    "### branch/tag history annotation ###\n",
    "#####################################\n",
    "\n",
    "def branchBackTracking(node):\n",
    "    parents = list(firstParentDags.predecessors(node[\"id\"]))\n",
    "    if len(parents) == 0:\n",
    "        return\n",
    "    parent = commitDic[parents[0]]\n",
    "    if \"branches\" in parent:\n",
    "        return\n",
    "    parent[\"branches\"] = node[\"branches\"]\n",
    "    node = parent\n",
    "    \n",
    "\n",
    "headCommit = commitDic[headId]\n",
    "\n",
    "# check head is origin/master. else quit\n",
    "if not \"master\" in headCommit.get(\"tags\", []) or not \"origin/master\" in headCommit.get(\"branches\", []):\n",
    "    print(\"HEAD IS NOT ORIGIN/MASTER OR MASTER\")\n",
    "    #quit()\n",
    "\n",
    "\n",
    "# origin/master\n",
    "node = headCommit\n",
    "while(True):\n",
    "    parents = list(firstParentDags.predecessors(node[\"id\"]))\n",
    "    if len(parents) == 0:\n",
    "        break\n",
    "    parent = commitDic[parents[0]]\n",
    "    parent[\"branches\"] = parent.get(\"branches\", [])  # branch가 없으면 branch에 빈 리스트 지정\n",
    "    parent[\"branches\"].append(\"origin/mrtk_development\")\n",
    "                               \n",
    "    node = parent\n",
    "\n",
    "\n",
    "# other branches\n",
    "for commit in commitList[:-1]:\n",
    "    if \"branches\" in commit:\n",
    "        branchBackTracking(commit)\n",
    "        \n",
    "        \n",
    "### insert all pairs path length into commit ###\n",
    "\n",
    "allPairsIterator = nx.all_pairs_dijkstra_path_length(dags, cutoff=10)\n",
    "allPairsDic = {}\n",
    "longestPathLength = 0\n",
    "print(\"\")\n",
    "print(\"Calculating pair length...\")\n",
    "for (idx, row) in enumerate(allPairsIterator):\n",
    "    allPairsDic[row[0]] = row[1]\n",
    "    longestPathLength = max(longestPathLength, max(row[1].values()))\n",
    "    if idx % 400 == 0:\n",
    "        print(\"iter \" + str(idx) + \" finished\")\n",
    "\n",
    "print(\"finished!!\")\n",
    "# print(allPairsDic[\"e163637d1537ccc9ccde1d25ca87b82286aa634d\"])\n",
    "\n",
    "# for key in allPairsDic.keys():\n",
    "#     print(key, len(allPairsDic[key].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:30:13.252570Z",
     "start_time": "2020-03-28T07:30:06.022481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Simialirty calculating...\n",
      "Total iteration : 10521 times\n",
      "Iteration 0 finished!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/POSTecHyeoN/Desktop/snu_HCI_Intern/githru/data/code/similarityCalLib.py:195: RuntimeWarning: divide by zero encountered in log10\n",
      "  return np.interp(np.log10(diffDays), (0, np.log10(totalCommitDays) ), (1, 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000 finished!!\n",
      "Iteration 2000 finished!!\n",
      "Iteration 3000 finished!!\n",
      "Iteration 4000 finished!!\n",
      "Iteration 5000 finished!!\n",
      "Iteration 6000 finished!!\n",
      "Iteration 7000 finished!!\n",
      "Iteration 8000 finished!!\n",
      "Iteration 9000 finished!!\n",
      "Iteration 10000 finished!!\n",
      "\n",
      "Finished!! Writing file...\n",
      "Success!!\n"
     ]
    }
   ],
   "source": [
    "scoreFullFileName = \"../log/\" + repo_name + \".score.json\"\n",
    "\n",
    "totalCommitDays = (parse(commitList[0][\"date\"]) - parse(commitList[-1][\"date\"])).days\n",
    "matrix = {}\n",
    "\n",
    "# ## DEBUGGING - TF IDF LOG\n",
    "# tfidfLog = \"\"\n",
    "\n",
    "print(\"Final Simialirty calculating...\")\n",
    "print(\"Total iteration :\", len(commitList), \"times\")\n",
    "for idx, commit in enumerate(commitList):\n",
    "    scoreList = {}\n",
    "    topoDistance = 1;\n",
    "     \n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Iteration \" + str(idx) + \" finished!!\")\n",
    "     \n",
    "    for idx_2, target_commit in enumerate(commitList):\n",
    "        try:\n",
    "            dist = allPairsDic[commit[\"id\"]][target_commit[\"id\"]]\n",
    "            if (dist < 0):\n",
    "                dist = dist * (-1);\n",
    "            if dist > topoDistance:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if dist >= topoDistance and dist != 0:\n",
    "            scoreDic = similarityCalLib.Scores()\n",
    "            scoreDic.calSimilarityScore(commit, target_commit, totalCommitDays, longestPathLength, allPairsDic[commit[\"id\"]])\n",
    "            subScoreList = {}\n",
    "            subScoreList[\"author\"] = round(scoreDic.author * 1000) / 1000\n",
    "            subScoreList[\"commitType\"] = round(scoreDic.commitType* 1000) / 1000\n",
    "            subScoreList[\"file\"] = round(scoreDic.file* 1000) / 1000\n",
    "#             subScoreList[\"commitDate\"] = round(scoreDic.commitDate* 100) / 1000\n",
    "#             subScoreList[\"branch\"] = round(scoreDic.branch* 100) / 1000\n",
    "#             subScoreList[\"cloc\"] = round(scoreDic.cloc* 100) / 1000\n",
    "            subScoreList[\"message\"] = round(scoreDic.message* 1000) / 1000\n",
    "#             subScoreList[\"topoDist\"] = round(scoreDic.topoDist* 1000) / 1000\n",
    "#             subScoreList[\"scoreSum\"] = round(scoreDic.scoreSum* 100) / 1000\n",
    "            scoreList[idx_2] =  subScoreList\n",
    "    \n",
    "    matrix[idx] = scoreList\n",
    "            \n",
    "            \n",
    "        \n",
    "#         if dist >= -5 and dist <= 5 and dist != 0:\n",
    "#             ### TFIDF LOG\n",
    "#             num = test(commit[\"tfidf\"], target_commit[\"tfidf\"])\n",
    "\n",
    "#             tfidfLog += str(num)\n",
    "#             tfidfLog += \"\\n\"\n",
    "#             ### EO DEBUGGING\n",
    "    \n",
    "    \n",
    "# print(\"\\nWriting TFIDF LOG...\")\n",
    "# tfidfF = open(\"../log/tfidf.csv\", \"w\")\n",
    "# tfidfF.write(tfidfLog)\n",
    "# tfidfF.close()\n",
    "# print(\"Success!!\")\n",
    "\n",
    "print(\"\\nFinished!! Writing file...\")\n",
    "f = open(scoreFullFileName , \"w\")\n",
    "f.write(json.dumps(matrix, indent=1, separators=(',', ':')))\n",
    "f.close();\n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:40:26.335672Z",
     "start_time": "2020-03-24T12:40:26.333191Z"
    }
   },
   "source": [
    "### (CURRENTLY OPTIONAL) T-SNE STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T07:30:13.256871Z",
     "start_time": "2020-03-28T07:30:13.254219Z"
    }
   },
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components = 2, verbose=1, perplexity=40, n_iter=300)\n",
    "# res = tsne.fit_transform(matrix)\n",
    "# res.tolist()\n",
    "\n",
    "# resList = []\n",
    "# for l in res:\n",
    "#     resList.append(str(l))\n",
    "# print(resList)\n",
    "\n",
    "# coFileName = \"../log/\" + projectName + \".co.json\"\n",
    "\n",
    "\n",
    "# f = open(coFileName , \"w\")\n",
    "# f.write(json.dumps(res.tolist(), indent=1, separators=(',', ':')))\n",
    "# f.close();\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
